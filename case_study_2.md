# Case-study оптимизации

## Актуальная проблема
Наша программа потребляет слишком много памяти, было решено снизить максимальный объем требуемой памяти до 70 Мб (73_400_320 байтов), за все время работы программы

## Формирование метрики
Главный показатель метрики у нас - объем потребляемой памяти.

Отчет callgrind 27129 показал, что в текущей реализации программа потребляет 2_136_042_440 байтов памяти, то есть, в 29 раз больше, чем мы хотим.
Для удобства и быстрого фидбэка будем работать не с полным файлом data_large.txt, а только с 1000 строк
Отчет callgrind 4097 показал, что для парсинга 1000 строк программа потребляет 130_660_088 байт памяти,
  нацелимся на результат меньше в 30 раз: 4_355_336 байт
Отчет callgrind 16284 показал, что для парсинга 10_000 строк программа потребляет 136_226_952 байт памяти,
  нацелимся на результат меньше в 30 раз: 4_540_898 байт





### Находка №1

Профилировщик memory_profiler с полным объемом данных завис, поэтому тоже запустим его с 1000 строк. Видим

`
Total allocated: 399.53 MB (3265157 objects)
Total retained:  257.90 kB (3522 objects)
`

Причем, 398.52 MB аллоцируется на 28 строке, это там, где мы парсим файл:
`
rows_count ? File.read('files/data_large.txt').split("\n").first(rows_count) : File.read(file_name).split("\n")
`

Аналогичный результат виден из отчета callgrind 4097, где сказано, что 99,73% памяти потребляется для метода String::split

Попробуем сделать "разбор" файла на лету, не засоряя память. 

Изменение кода типа 

`
file_lines = parse_file(file_name, rows_count)
file_lines.each { |line| parse_line(line) }
def parse_file(file_name, rows_count)
  rows_count ? File.read('files/data_large.txt').split("\n").first(rows_count) : File.read(file_name).split("\n")
end
`
на код типа

`
parse_file(file_name, rows_count)
def parse_file(file_name, rows_count)
  if rows_count
    File.read('files/data_large.txt').split("\n").first(rows_count).each { |line| parse_line(line) }
  else
    File.read(file_name).split("\n").each { |line| parse_line(line) }
  end
end
`
не дало никаких результатов. Памяти потребляется ровно столько же (отчет callgrind 6189)

Можно попробовать два варианта сокращения используемой памяти: использовать each_char для всей строки прочитанного файла, чтобы избежать вызова `split`, и использовать `shift`.
может быть, это даст уменьшение использования памяти. Но сперва оценим, сколько мы потеряем в производительности из-за этого:
сравним в benchmark_comparing.rb 
`
read_file_with_split
read_file_with_each_char
read_file_with_shift
`
`
Получаем результат:
read_file_with_split:    39062.0 i/s
read_file_with_shift:    38009.5 i/s
  1.03x  (± 0.01) slower
read_file_with_each_char:     6156.5 i/s
  6.34x  (± 0.07) slower
`
Вариант с each_char, как видно, нам сразу не подходит. А shift всего на 3 сотых процента медленнее each, его можно применить.
Отчет 17192 показал, что теперь на String::split по-прежнему требуется 130_037_640 - почти никакго изменения, кроме лишь того, что в процентном соотношении это теперь 100%

Поставил в программу Benchmark.realtime, обернул в него этот проблемный кусок кода и попробовал поэкспериментировать с количеством строк, вызываемых для парсинга. Отслеживать результаты стал на парсинге одного миллиона строк.

<!-- Первый вариант -->
`
file_lines.each { |line| parse_line(line) }
`
<!-- Второй вариант -->
`
while file_lines.count > 0
  line = file_lines.shift
  parse_line(line)
end
`

И первый и второй вариант тут показывали примерно одинаковое поведение - чаще в районе 650 MB, иногда в районе 750 - 790 MB памяти. Тогда я включил строку с активацией Garbage коллектора
`
GC.start(full_mark: true, immediate_sweep: true)
`
И результаты стали более стабильными: в первом случае - в районе 640-650 MB, во втором - 620-640 MB. Время выполнения практически одинаковое.

В отчете qcachegrind 14900 мы видим, что 130_037_640 байт тратится на каждый из двух вызовов parse_file и всего по 273 байта - на каждый из 10_000 вызовов parse_line. И все это при вызове функции split. Попробуем все-таки от нее отказаться, чтобы посмотреть, как изменится потребление памяти.

Сперва в экспериментальных целях я сам сплит строк файла вынес в benchmark_copmaring и туда же поместил его аналог с помощью побуквенного разбора строки. Получилось, что чтение файла требует 129 МБ (не очень понятно, как тогда выполнить задание получится), а дальше мы видим, что для сплита нужно порядка 250 МБ памяти, а для побуквенной итерации - около 140 МБ. 

rss before file read: 12 MB
rss after file read: 141 MB
rss before file splitted: 141 MB
rss after file splitted: 395 MB
rss before empty string and array_lines created: 395 MB
rss after array_lines filled: 531 MB

Значит, попробуем аналогичным образом переделать обработку файла в нашем методе.


<!-- Стало -->
ruby task-1.rb
Start
rss before array allocation: 13 MB

Making full GC...
rss after concatenation: 817 MB
Finish in 49.17
ruslan@MacBook-Pro-Ruslan rails-optimization-task1 (Valeev_task_1) $
<!-- Было -->
ruslan@MacBook-Pro-Ruslan rails-optimization-task1 (Valeev_task_1) $ ruby task-1.rb
Start
rss before array allocation: 13 MB
Making full GC...
rss after concatenation: 873 MB
Finish in 10.25


Настолько жертвовать производительностью для нашего бюджета непозволительно, да и профит совсем не многообещающий. 
Я решил немного изменить чтение файла, и попробовать другие ruby методы для чтения файла построчно
Но использование, например, 
File.readlines показало 1084 MB 

Наконец-то нашел способ "экономно" открыть файл и построчно его просканить. Возможно, на самом деле, и предыдущие способы были экономные, но я замерил сейчас само открытие и построчные итерации без вызова на них дальнейших методов, и получились без раздутия памяти

    file = open file_name
    puts "rss after file opening: #{print_memory_usage}"
    while line = file.gets
      # parse_line(line)
    end

Теперь тест не прошел, но это из-за отсутствия chomp в конце строки file.gets&.chomp - теперь тесты опять позеленели, 
а отчет callgrind 41204 показал, что за все время работы файла мы используем 2_266_079_920 Мб. 39% опять пожирает метод split, но уже в рамках парсинга строки. Попробуем теперь отслеживать метрику вокруг этого парсинга.


Попробовал переписать все в один метод, не используя вызовов других методов, а также с наименьшим числом переменных, но использование памяти даже выросло до 850 Мб местами. Подключился к слак каналу, пока там не особо много мне ответили на мой запрос о помощи, но также я посмотрел видео с RubyRussia, и, думаю, есть что применить оттуда в моем задании. Попробую переписать метод.

Сегодня в чате посоветовали записывать файл построчно. Я проверил в benchmark_comparing.rb, перекопировать вот так построчно весь наш огромный файл, и, в общем то стало ясно, что это действительно работает. 

Start
Memory using before open file for read 13 MB
Memory using after open file for read 13 MB
Memory using before create file for write 13 MB
Memory using after create file for write 13 MB
Memory using after rewrite file from file_read to file_write 8 MB
Memory using after close file_write 8 MB
Memory using after deleting file_write 8 MB
Making full GC...
rss after GC: 8 MB

файл размером 134,4 Мб можно скопировать только с использованием 13 Мб. 

Попробуем теперь перенастроить нашу программку на такой же принцип, не аккумулируя промежуточные данные в переменных.
Думаю, можно статистику по юзерам записывать во временные файлы, а статистику суммарную - хранить в переменных.
Для начала попробуем упростить отчет по каждому юзеру до редактируемых форматов, чтобы можно было легко править данные, не вычленяя число из строки каждый раз, чтобы поменять, например, '142 min' в суммарной длине сессий на '143 min'
Нашел, мне кажется, решение получше. Юзеры, как я выяснил, встречаются только единожды, а вот сессии по ним - неопределенное количество раз. Также я провел эксперимент, и выяснил, что с помощью grep можно прямо файлу будет находить сессии для конкретного юзера и парсить уже их. Не успел сегодня метрику по этому решению замерить, но думаю, это неплохой компромисс между экономией времени и памяти.


Сегодня построил потоковую запись, но время выполнения выросло в разы. Притом, что я отказался от первоначальной идеи записывать каждого юзера во временный файл, а сразу стал записывать в итоговый файл. Завтра буду перфоманс опять выравнивать.

По-видимому, поиск по файлу занимает слишком много времени (метод grep), попобуем все-таки исходить из того, что записи сессий идут в файле сразу после их юзеров (без поиска и сопоставления сессий и юзеров по id)

Привел в порядок формирование JSON файла, теперь он формируется так, чтоб потом его можно было парсить, также написал тесты на корректность его формирования без учета порядка полей.

Запустил профилирование с отчетом 32482, но этот отчет показывает, что памяти потребляется даже больше, чем поначалу, 2_676_038_440. Но при этом в самом методе потребление памяти несколько раз показывается через puts, в том числе в самом конце, и эти показатели совсем иное показывают.
Memory using BEFORE ALL 13 MB
Memory using after parse lines 16 MB
Memory using AFTER ALL 16 MB

Посмотрим, что покажет valgrind
