# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: количество память показанное memory_profiler на 30_000 строк

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 30-40 сек

Вот как я построил:
- прогон программы, фиксирование потребляемой оперативной памяти в конце программы
- прогон профайлерами, фиксирование точки роста
- изменения в коде
- тесты


## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался memory_profiler, stackprof, ruby-prof

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- memory_profiler на 30_000 строк показывал 3.78 гб аллоцированной памяти.
- переписать программу иным способом, а именно, не созранять всех юзеров и сессии в памяти, а на лету за один проход все посчитать, собрать report и записать в файл
В первой итерации я так и сделал, так как было понятно, что программа в первоначальном виде не имеет шансов уложится в бюджет по памяти.
Переписал программу "в лоб", не особо задумываясь об потреблении памяти, просто чтобы работало как и было и за один проход по исходному файлу составляла report.
- memory_profiler на 30_000 строк показал результат 43.12 mb (при этом время выполнения программы без профилирования на data_large.txt стало 25 секунд, что меньше, чем лучший результат после оптимизации по CPU)
- главная точка роста стала в String#split

### Ваша находка №2
- ruby prof c RubyProf.measure_mode = RubyProf::MEMORY показал главную точку роста в String#split (52 процента)
- попробовал сделать с блоком в split, чуть уменьшив каждый массив для сессий и юзеров
- memory_profiler на 30_000 строк показал результат 41 mb
- String#split все еще на первом месте, но рядом уже sort_by по строкам дат

### Ваша находка №3
- ruby prof c RubyProf.measure_mode = RubyProf::MEMORY показал главную точку роста в Enumerable#sort_by (31 процент)
- использовал SortedSet
- memory_profiler на 30_000 строк показал результат 41 mb
- Enumerable#sort_by пропал из топа

Еще добавил magic comment.
Поменял где надо массивы на Set.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы написал performance rspec тест
