# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: кол-во выделяемой памяти, требуемой для обработки 10000 строк.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за пару минут.

Вот как я построил `feedback_loop`:
- провожу профилирование, нахожу точку роста
- вношу изменений
- прогоняю тесты
- прогоняю бенчмарки
- коммичу сделанные изменения

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался профилировщиками: memory_profiler, ruby_prof, stackprof.

Вот какие проблемы удалось найти и решить:

### Ваша находка №1
- ruby_prof в режиме callstack и memory_profiler указывали, что больше всего объектов аллоцирует метод Date#parse
- убрал вызовы парсинга даты, тк дата уже приходит в нужном формате
- погрешность метрики была очень высокая, поэтому сложно оченивать эфективность, но возьму средние значения: метрика уменьшилась с ~72 MB до ~70 MB
- данный метод перестал быть главной точкой роста

### Ваша находка №2
- stackprof и memory_profiler указывали, что больше всего объектов аллоцирует метод String#split
- решил убрать лишние вызовы методов String#split
- как бы это не странно, но метрика увеличилась с ~70 MB до ~74 MB, почему так произошло мне не очень понятно
- профилировщик так же указывал на метод String#split как на одну из основных точек роста, но оставался только один split в цикле парсинга данных.

### Ваша находка №3
- после двух пердыдущих попыток, решил использовать для нахождения точек роста memory_profiler, не через статистику аллокации объектов, а через статистику аллокации количества памяти. memory_profiler указывал на конструкции добавления элементов в массив через метод +  
- переделал добавлние элементов через метод << 
- метрика уменьшилась с ~74 MB до ~44 MB
- memory_profiler больше не указывал на данный участок кода

### Ваша находка №4
- memory_profiler указывал, что больше всего памяти выделяется на сточке user_sessions = sessions.select { |session| session['user_id'] == user['id'] }
- перенес формирование сессий пользователя в основной цикл обратобтки данных
- метрика изменилась с ~44 MB до ~42 MB
- memory_profiler больше не указывал на данный участок кода

### Ваша находка №5
- memory_profiler указывал на метод формирования списка объектов пользователей users_objects = users_objects + [user_object]
- переделал добавлние элементов через метод <<
- метрика изменилась с 42 MB до 33 MB
- memory_profiler больше не указывал на данный участок кода

### Ваша находка №6
- ruby_prof, stackprof и memory_profiler указывали на метод Object#collect_stats_from_users, а конкретно на неоднократный вызов метода Array#each в нем
- убрал использование блоков, перенес весь код непосредсвенно в метод Object#collect_stats_from_users, сейчас все операции начали выполняться за один цикл
- метрика изменилась с 33 MB до 32 MB
- метод Object#collect_stats_from_users перестал быть главной точкой роста

### Ваша находка №7
- все профилировщики указывали на методы Array#each, которых было несколько. А так же на метод String#split, который вывзывался в основном цикле обработки данных. Из этого сделал вывод, что в циклах накапливается большой объем памяти. Принял решение отрефакторить программу под потоковый подход. 
- полностью переписал код)
- метрика изменилась с 32 MB до 22 MB

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику обработки 10000 строк с ~72 MB до 22 MB. Метрика для полного объема данных так же составляет ~22 MB, что говорит о том, что память больше не накапливается.
Время выполнения обработки полного объема данных после изменений составляет ~11s, в первом ДЗ мой результат был ~33s.
На самом деле я не до конца осознал как работать с метрикой при небольших изменениях, тк погрешность очень большая. GC ведет себя не очень непредсказуемо и иногда метрика после изменений больше, нежели до. Возможно я делал что-то неверно.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы я написал тест c использованием матчера perform_allocation от rspec с проверкой, что 10000 строк обработаются с бюджетом 10мб.
