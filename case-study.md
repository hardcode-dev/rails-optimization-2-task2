# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я решил использовать такую метрику: потребляемая программой память в МБ

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`:
* Для начала определился с комфортным для тестов размером файла - 10к строк
* далее переписал чтение и обработку файла построчно (признаюсь честно, подход получился не сильно итеративным, поскольку лекцию смотред внимательно, получилось неплохо сразу :) )
* Далее идет итеративный процесс оптимизации:
  * Замерял потребление памяти до изменения
  * Строил отчеты разными профилировщиками
  * Вносил изменения
  * Замерял потребление памяти после имзенения

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался следующими инструментами:
* `ruby_prof` 
* `stack_prof`
* `memory_profiler`

в основном пользовался `ruby_prof` и `memory_profiler`

Вот какие проблемы удалось найти и решить

### находка №1
- общее потребление памяти составило 80 MB
- `memory_profiler` показал, что общее количество аллоцированной памяти до внесения изменений для 10к строк составляет 447 MB
- главная точка роста - увеличение массива сессий
- для начала решил (как и написано в задаче) переписать код в потоковый режим работы

### находка №2
- размер файла увеличил до 100к строк
- все строки сразу вынес в константы, использовал bang! - методы
- общее потребление памяти составило 28 MB
- `memory_profiler` показал, что общее количество аллоцированной памяти до внесения изменений для 100к строк составляет 112 MB
- главная точка роста - разделение строки методом `split`
- решил попробовать использовать `split` с блоком

### находка 3
- размер файла увеличил до 100к строк
- все строки сразу вынес в константы, использовал bang! - методы
- общее потребление памяти составило те же 28 MB
- главная точка роста перестала быть проблемой
- `memory_profiler` показал, что общее количество аллоцированной памяти до внесения изменений для 100к строк составляет 92 MB

### находка 4
- все строки сразу вынес в константы, использовал bang! - методы
- особой разницы разницы по отчету `ruby_prof` заечено не было 
- результат меня устроил - магия какая-то :)

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с `не знаю, сколько там было для всего файла` до 28МБ для всего файла и уложиться в заданный бюджет.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы написал тест на потребление памяти
