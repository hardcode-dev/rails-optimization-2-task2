# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.
Необходимо было обработать файл с данными, чуть больше ста мегабайт.
У нас уже была программа на `ruby`, которая умела делать нужную обработку.
Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: преобразование текстового файла в файл json предложенного формата с потреблением программой менее 70Мб оперативной памяти на всем протяжении работы.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop

Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 2-3 минуты:

-произвести замер времени обработки файла и количества потребляемой программой памяти
-произвести профилирование и выявить точку роста
-попытаться использовать найденную точку роста чтобы оптимизировать код
-выполнить тестирование корректности и скорости обработки файла
-произвести повторный замер времени и памяти, чтобы убедиться, что оптимизация помогла

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы эффективно работать над оптимизацией, я предварительно нарезал исходный файл на файлы, содержашие 10_000, 100_000б 500_000 и 1_500_000 строк, чтобы по мере улучшения метрики программы использовать все более и более большой файл.

Для того, чтобы найти "точки роста" для оптимизации я воспользовался пропилировщиками memory-profiler, rubyprof в режимах профилирования памяти и аллокаций, stackprof, а так же инструментом valgrind massif, позволяющем оценить количество потребляемой программой памяти на всем протяжении ее работы.

Вот какие проблемы удалось найти и решить

### Подготовка программы к оптимизации
Перед тем, как начать профилировать программу, я переписал ее в потоковый режим работы с исходным файлом, т.к. исходный файл для обработки уже превышает размер метрики и не позволяет снизить лимит потребления оперативной памяти при загрузке файла целиком. Я постарался сразу перенести улучшения, полученные в ходе оптимизации времени обработки файла (как например устранение дублирующих `upcase`ов и излишнего парсинга даты). Переписанную программу проверил тестом, чтобы удостовериться, что я не нарушил ее работу. Так же я написал тест, защищающий количесво потребляемой программой памяти от регрессии.


### Находка №0
- самой интересной находкой было то, что я сразу добился 37Mb оперативной памяти в конце работы программы при обработке файла `data_large.txt`. Но время обработки файла оставляло желать лучшего. Примерно половина исходного файла обрабатывалась 60 секунд. Построение графика использования памяти valgrind-massif-visualiser'ом не представлялось возможным, т.к. даже на построение графика для 100_000 строчного файла уходило более минуты. Поэтому я решил продолжить профилирование в надежде улучшить и этот показатель. Пример неоптимизированной переписанной программы находится в файле `streaming_task.rb`.

### Находка №1
- benchmark выдает MEMORY USAGE: 20 MB и Finish in 2.47 для 100_000 строк.
- memory-profiler показывает что всего аллоцируется 372.83 MB и большая часть (153.68 MB) в строке `File.write('result.json', "\"#{@full_name}\":#{user_stats[@full_name].to_json}#{closing_char}", mode: 'a')` а самым прожорливым объектом является `File`
- я подумал, что возможно `File.write` каждый раз открывает файл `result.json` и пишет в него и решил переписать прогрумму так, чтобы файл открывался лишь в начале работы, данные писались в него в процессе и файл закрывался, после обработки всех данных.
- benchmark выдал MEMORY USAGE: 13 MB и Finish in 1.04 для 100_000 строк, а для полного файла MEMORY USAGE: 13 MB
и 32.4 sec.

### Находка №2
- memory-profiler сказал что теперь всего аллоцируется 229.13 MB MB и большая часть (71.71 MB) в сроке `@session_dates << fields[5].gsub(/\n/,"")` а самый аллоцируемый объект теперь `String`. Проведя профильрование с помощью RubyProf в режиме MEMORY, я увидел, что действительно 37% занимаемой памяти приходится на метод `parse_session` и вызов `gsub` внутри метода, который отрезал символ перевода строки у даты.
- я решил попробовать воспользоваться методом `chomp`, что дало MEMORY USAGE: 13 MB и 25.26 sec
- профилировщик больше не показывал максимальное потребление памяти в этой строке.

### Находка №3
- memory-profiler сказал что теперь всего аллоцируется 160.80 MB MB и большая часть (71.71 MB) в сроке `cols = line.split(',')` а самый аллоцируемый объект все так же теперь `String`. Я решил проверить так ли это и при профилировании с помошью Stackprof увидел, что действительно внутри `File.foreach` самая тяжелая операция это метод `split` преобразующий строку в массив, но оптимизировать его не представлялось возможным.

### Прочие улучшения
- замена библиотеки `json` на более оптимизированную `oj` не дало ощутимого результата 13 MB/22.9 sec. Но это улучшение было оставлено т.к. не испортило общих показаний памяти и скорости
- добавление магического комментария `frozen_string_literal: true` снизило количество аллоцируемой памяти с 160 (95 еа строки) до 140 (77 на строки)
- Избавление от дублируюших split'ов дало 96.11 MB общей аллокацию памяти
- Понижение уровня глубины хеша `user_stats` принесло MEMORY USAGE: 7 MB
- Чтобы не плодить сущности, там где это возможно, простые методы были заменены на бэнг-методы (`sort, reverse`)

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с нескольких сотен мегабайт (было невозможно проверить метрику для полного файла на неоптимизированной программе, без потокового чтения и записи) до 37Mb в максимуме и уложиться в заданный бюджет.
