# Case-study оптимизации

## Актуальная проблема

В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики

Для того чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: потребление менее 70мб при обработке datalarge файла и меньше 30 секунд

## Гарантия корректности работы оптимизированной программы

Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop

Для того чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *5 секунд*

Вот как я построил `feedback_loop`: решил разделить запуск тестов и запуск проверки производительности в разных файлах, а разграничивать передачу файла через передачу аргумента. От автоматического запуска теста при сохранении отказался, т.к. автоматический трекинг изменений и запуск тестов немного раздражает.

В итоге получилось:

- profile
- modify
- tests
- benchmark уже кастомным профайлером ибо стандартные оч много ресурсов жрут и нужны только для осознания точек роста
- commit

## Вникаем в детали системы, чтобы найти главные точки роста
Начал сначала с оригинального файла, но понял что по отчетам начал опять оптимизировать и находить те места что и в прошлой задаче. Поэтому решил отталкиваться уже от оптимизированной по cpu задачи.

Из отчетов основным выбрал для себя memory_profiler и ruby-prof в режиме caиlstack самые информативные и быстрые как по мне. CallTree `c визуализацией в `QCachegrind очень красив и удобен, но в целом большого профита не вижу для себя в отличии от cli инструментов. Да и в целом как-то cli более привычные.

Первый запуск для формирования отчета, что бы было от чего отталкиваться. На сеплиревонных данных в 100к записей

```
Time: 8.55
Memory before: 20.99 MB
Memory after: 841.82 MB
Total allocated: 118.06 MB (1462536 objects)

allocated memory by class
-----------------------------------
  56.81 MB  String
  38.06 MB  Array
  23.11 MB  Hash
  72.00 kB  Date
  16.85 kB  File
  240.00 B  Proc
  224.00 B  JSON::Ext::Generator::State
   72.00 B  Thread::Mutex
   40.00 B  Set
```

### Гипотеза №1

Попробовать избавится от загрузки всего файла в память. вместо readlines использовать foreach. Думаю что это может являться одной из причин, но не самой главной на самом деле.

```
Time: 8.97
Memory before: 21.29 MB
Memory after: 842.17 MB
Total allocated: 117.04 MB (1462537 objects)

allocated memory by class
-----------------------------------
  56.81 MB  String
  37.04 MB  Array
  23.11 MB  Hash
  72.00 kB  Date
  16.85 kB  File
  240.00 B  Proc
  224.00 B  JSON::Ext::Generator::State
  136.00 B  Enumerator
   72.00 B  Thread::Mutex
   40.00 B  Set
```

Результат нулевой по отчету профайлера :)

Но если отдельно замерять в рамках запуска файла без всяких профайлеров, то результат есть, и даже быстрее на 2 сек

Было

```
➜ ruby task-2.rb data_large.txt
Time: 28.36
Memory before: 20.15 MB
Memory after: 2820.75 MB
Memory real: 2800.59 MB
```

Стало

```
➜ ruby task-2.rb data_large.txt
Time: 26.09
Memory before: 20.55 MB
Memory after: 2411.81 MB
Memory real: 2391.27 MB
```

Вывод: надо не только читать потоком, но и писать в файл тоже потоком. Ибо data_large.txt занимает 128M что означает у нас никогда не получится вписаться в метрики если мы будем грузить его в память для процессинга, а еще и расход на интерпретатор порядка 20мб будет.

### Гипотеза №2

Переписать формирование отчета на потоковый стиль. Спасает тот факт что у нас все сессии с сортировкой по юзеру, что означает не надо сильно костылить вариант того что у нас может быть ситуация когда все данные сессий будут находиться вперемешку.

Можем формировать в памяти отчет по сессиям конкретного юзера и сбрасывать его потоком в файл, позже очищать память  удаляя уже сохраненные данные из памяти и сбрасывая их в файл до тех пор, пока не закончится отчет который нужно парсить.

```
Total allocated: 11.89 MB (149261 objects)

allocated memory by class
-----------------------------------
   6.29 MB  String
   3.64 MB  Array
   1.95 MB  Hash
  16.85 kB  File
  160.00 B  Proc
  136.00 B  Enumerator
   72.00 B  Thread::Mutex
   40.00 B  Set

➜ ruby task-2.rb data_large.txt
Time: 11.38
Memory before: 19.86 MB
Memory after: 23.35 MB
Memory real: 2.46 MB
```

Результат порадовал, сразу же вписался во все необходимые метрики.

PS: ШОК: пока пытался руками воспроизвести структуру отчета, заметил невероятное, мы брали дату в уже нужном формате и потом ее приводили опять к этому же формату. Просто невообразимое рука лицо.... аж стыдно стало что упустил столь очевидный момент, зарылся в изучения того как эффективно парсить, вместо того чтоб посмотреть, а что и во что я там парсить должен, прежде чем оптимизировать надо голову хоть немного да включать)

PSS: Осталась одна точка роста в виде split. Но я решил ее не оптимизировать, иначе читаемость кода ухудшится в разы, читаемость кода важнее чем экономить мегабайты и очень жестко заниматься  оптимизацией в момент когда метрики достигнуты

## Результаты
Удалось существенно оптимизировать метрику по потребляемым ресурсам.  с 2.8гб потребления памяти до 20мб а так же ускорить временную метрику с 28сек до 12сек (на быстром маковском интел i7) и 18 сек и 40-45мб на линуксе на амд.

Чтоб не страдать от установок Valgrind воспользовался ноутбуком на адм результаты конечно хуже, чем на маке в силу отсталости техпроцесса, но все еще в рамках метрики остался. Добился константного потребление памяти и оно находится всегда на одном уровне в 40мб, с мелким пиком в 45мб при закрытии дескриптора файла в конце, что означает мы можем масштабировать отчет и при этому не допустить разбухания памяти, считаю это важно

Еще из сюрпризов для себя открыл что File.foreach с использованием line.split(',') быстрее чем CSV.foreach

Немного занимательных стетей и видосов по теме:
https://usilenie.plus/2021/grand-theft-cpu/
https://www.youtube.com/watch?v=n43O0u77d8o&ab_channel=Contentful



## Защита от регрессии производительности
Написал простой тест который считает аллокацию на семплированных данных.

Учитывая что теперь программа работает в потоковом режиме то она потребляет одинаковое количество памяти вне зависимости от размера входных данных.


Ну и результат:

```
Time: 11.38
Memory initial script: 19.86 MB
Memory after parsing: 23.35 MB
```
