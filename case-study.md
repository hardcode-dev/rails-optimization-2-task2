# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: *тут ваша метрика*

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`: *как вы построили feedback_loop*

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *инструментами, которыми вы воспользовались*

Вот какие проблемы удалось найти и решить

### Ваша находка №1
Был велик соблазн просто скопипастить решенную 1-ую домашку и начать профилировать её, но решил все же пройти этот путь с самого начала.
10k строк

416.75 MB  Array
15.22 MB   String
7.09 MB    Hash

287.67 MB  optimizations-course/rails-optimization-task2/task-2.rb:54
sessions = sessions + [parse_session(line)] if cols[0] == 'session'

Пробуем заменить на sessions << parse_session(line) if cols[0] == 'session' и замечаем, что после этой оптимизации действительно стало куда меньше объектов класса Array

129.58 MB  Array
15.22 MB   String
7.09 MB    Hash

### Ваша находка №2
10k строк

129.58 MB  Array
15.22 MB   String
7.09 MB    Hash

104.07 MB  optimizations-course/rails-optimization-task2/task-2.rb:100
user_sessions = sessions.select { |session| session['user_id'] == user['id'] }

Снова неоптимальный подход к нахождению сессий для каждого юзера, да и в целом тендеция топа, те же самые проблемы, которые были в прошлом домашнем задании, правим сразу скоупом добавляя при этом ```# frozen_string_literal: true```
Все сразу приобразилось и стало куда лучше по памяти

6.51 MB  String
3.34 MB  Array
1.22 MB  Hash

по итогу, программа занимает 20мб при 10k строках

### Ваша находка №3
На данный момент, главная проблема заключается в том, что мы храним все в памяти, а должны поточно писать В файл, правим.
Переписав всю программу на поточную запись, потребление памяти при парсинге большго файла не выходит за рамки 20мб. Попробауй оптимизировать дальше

### Ваша находка №4
Попробовал в очередной раз заменить .to_json, на Oj, кажется стало даже больше потреблять памяти на +-3 мб

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с *того, что у вас было в начале, до того, что получилось в конце* и уложиться в заданный бюджет.

То ли мне так везет, то ли я хз, но как-то Oj не помог. Но хорошая новость в том, что научился пользоваться разными профилировщиками, больше всего зашел memory_profiler и ruby-prof с его graph отчетом(в принципе, как и в прошлом дз)

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*
Обычный тест, которй проверяет на memory usage.