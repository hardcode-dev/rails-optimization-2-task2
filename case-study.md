# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: необходимо, чтобы программа в процессе работы потребляла не более 70МБ
оперативной памяти и при этом отрабатывала в течение 30 секунд

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за ~ 4 - 5 часов.

Вот как я построил `feedback_loop`:
1) Я вынес тест правильности работы программы в отдельный файл.
2) Написал программу run_time_benchmark.rb, которая на нескольких тестовых файлах меньшего размера
и в т.ч. на целеовм будет показывать время работы программы.
3) Написал программу run_memory_benchmark.rb, куда из тела основной программы вынес замер используемой
памяти. Программу сделал многопоточной, чтобы разные процессы программы не влияли друг на друга.
4) Написал программу run_prof, которой буду профилировать работу как по процессору так по аллокациям
и по памяти.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof и valgrind.
Особенно нравится ruby-prof из-за удобства использования. valgrind полезен только лишь чтобы
смотреть график на пиковые значения. Но по СИ файлам навигировать там бестолку.

Вот какие проблемы удалось найти и решить

### Моя находка №0

В начале пути снял метрики по памяти "в пути" c помощью валгринд:
В пике программа на 10К строк использует 160МБ памяти.

### Моя находка №1
Для начала переведем программу на построчное чтение файла и исполнение для каждого пользователя.
Придется почти везде писать в файл самостоятельно, так как вся стата по пользователям должна лежать
внутри usersStats. Еще будем индуктивно накапливать сессии в пользователя.

- как изменилась метрика
  Согласно валгринд теперь даже на data_large файле программа использует около 50МБ,
  ровно как на 100К и 1М. График валгринд параллелен оси времени. Можно предположить, что программа
  стабильно будет обрабатывать любой объем. Дальше будем работать над временем исполнения.
  После первого этапа имеем:

  Память:
    Память на data_large: 50,4 МБ
  Время:
    На 100К строк: 1.5777166379994014 с.
    На 1M строк: 19.703421123000226 с.
    На data_large строк: 59.18999095599975 с.

### Моя находка №2
- какой отчёт показал главную точку роста
  ruby-prof allocations
- как вы решили её оптимизировать
  Уберем парсинг дат. Еще в прошлом задании он оказался не нужен. Но по аллокациям - он главная точка.
- как изменилась метрика

  Память:
    Память на data_large: 50,4 МБ. Т.е. не изменилась. Строится долго. Буду смотреть на последнем шаге.
  Время:
    На 100К строк: 0.9166418589993555 с.
    На 1M строк: 9.432495405000736 с.
    На data_large строк: 30.176775833000647 с.

- как изменился отчёт профилировщика
  Парсинг дат больше не точка роста по аллокациям.

### Моя находка №3
- какой отчёт показал главную точку роста
  ruby-prof wall
- как вы решили её оптимизировать
  Оптимизируем flush пользователя. По wall_time это главная точка, по аллокациям вторая.
  Не будеем накапливать сессии в пользователя. Будем в процессе обработки файла сразу индуктивно
  собирать нужную инфу
- как изменилась метрика
  Время:
    На 100К строк: 0.7506896089998918 с.
    На 1M строк: 8.526422361999721 с.
    На data_large строк: 26.138684407000255 с.

- как изменился отчёт профилировщика
  Парсинг дат больше не точка роста по аллокациям.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.

При измерении через "ps -o" имеем следующие результаты по памяти:
  For data_100K.txt:
  MEMORY USAGE: 36 MB
  For data_1M.txt:
  MEMORY USAGE: 36 MB
  For data_large.txt:
  MEMORY USAGE: 36 MB
При измерении времени имеем результаты:
  На 100К строк: 0.7506896089998918 с.
  На 1M строк: 8.526422361999721 с.
  На data_large строк: 26.138684407000255 с.

В метрику и бюджет уложились.
Добавил файлик valgrind_finish.png, где видно, что результат чуть более 50 МБ. 

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы написаны тесты:

1) Привел в порядок функциональный тест.
2) Написал тест на время, на 100К строковый файл. Единственное, чего не понимаю, почему-то в
тестовой среде на этом файле имею время ~ 1,2 секунды. Если же просто запускаю через run_time_benchmark,
то стабильно укладываюсь в 0,8.
3) Написал тест на используемую память. Меряю память в форкнутом процессе. Понимаю, что в
форкнутый процесс копируется вся память, что была до исполнения, но кмк так мерить с точки
зрения философии правильнее, чтобы текущий процесс не давал наводки на измеряемый. Тем более это
на мой взгляд было важно в файле run_memory_benchmark.
