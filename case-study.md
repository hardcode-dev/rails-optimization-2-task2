# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: ruby-prof(CallTree) и MemoryProfiler
## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`:
- проверить потребляемую память после выполнения с помощью puts "MEMORY USAGE: %d MB" % (ps -o rss= -p #{Process.pid}.to_i / 1024)
- проверить потребляемую память после выполнения с помощью memory_profiler / ruby-prof calltree
- поиск главной точки роста
- изменение кода


## Вникаем в детали системы, чтобы найти главные точки роста
Изначально я переписал формирование отчета на построчное чтение файла.
Для тестирования использую файл в 100_000 записей
MemoryProfiler.report
Total allocated: 158386517 bytes (2582259 objects)
Total retained:  10638740 bytes (161867 objects)

Для того, чтобы найти "точки роста" для оптимизации я воспользовался ruby-prof(CallTree) и MemoryProfiler

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- отчеты `ruby-prof` показали главную точку роста в методах `String#split` и  `Array#sort`
- рефакторинг кода, вынос сортировки браузеров из общего цикла чтения файла
- было: 158 MB, стало 123 MB для 100_000 строк
- исправленная проблема перестала быть главной точкой роста

### Ваша находка №2
- отчет `stackprof` показал главную точку роста `String#split, String#upcase, Integer#to_s`
- рефакторинг
- 88 MB -> 86 MB
- Нет идей, что еще можно сделать


## Результаты
Исходный репорт:
MemoryProfiler.report
Total allocated: 158386517 bytes (2582259 objects)
Total retained:  10638740 bytes (161867 objects)

Результат:
MemoryProfiler.report
Total allocated: 86365749 bytes (1240026 objects)
Total retained:  12357924 bytes (177091 objects)

Время обработки по идогам предыдущего задания
- Метрика:
   ```
  100_000 lines - 18.817424 sec.
  
После оптимизации памяти
- Метрика:
   ```
  100_000 lines - 0.816592  sec.

В результате проделанной оптимизации наконец удалось обработать полный файл с данными файл с данными.
- Метрика:
   ```
  MEMORY USAGE: 3217 MB
  data_large.txt 30.208056 sec

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*
