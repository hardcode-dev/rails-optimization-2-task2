# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Порядок роста используемой помяти в начеле оптимизации

2025-02-01 15:17:52 +0300: 25 MB 
2025-02-01 15:17:53 +0300: 139 MB 
2025-02-01 15:17:54 +0300: 147 MB 
2025-02-01 15:17:56 +0300: 155 MB 
2025-02-01 15:17:57 +0300: 166 MB 
2025-02-01 15:17:59 +0300: 177 MB 
2025-02-01 15:18:00 +0300: 186 MB 
2025-02-01 15:18:02 +0300: 196 MB 
2025-02-01 15:18:03 +0300: 205 MB 
2025-02-01 15:18:05 +0300: 227 MB 
2025-02-01 15:18:06 +0300: 237 MB 
2025-02-01 15:18:08 +0300: 245 MB 
2025-02-01 15:18:09 +0300: 256 MB 
2025-02-01 15:18:10 +0300: 274 MB 
2025-02-01 15:18:12 +0300: 290 MB 

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: программа затрачивает на обработку больших файлов не больше `70MB`
Перед оптимизацией программа затрачивает на обработку файла в 50000 строк 335 MB

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`:

1. Сформировал короткие удобные алиаасы, что бы быстро получать обратную связь
2. Быстро запустил профилировщики(`make all_reports`) -> посмотрел отчеты, нашел точку роста -> внес изменения и снова запусти профилировщик
3. Создал отдельный вотчер, который запускается в отделном треде и проверяет количество используемой памяти(пишет лог), если оно привышает пороговое значение то выбрасывает исключение 

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался *инструментами, которыми вы воспользовались*

Вот какие проблемы удалось найти и решить

### Ваша находка №1
1. отчет `memory prof` показал что основная точка роста которая потребляет много памяти это соединение двух массивов при обходе строк(на файле в 10000 строк потребление памяти 287 MB, общее потребление 440.21 MB)
2. Заменил `users + [parse_user(line)]` на `users << parse_user(line)`
3. Метрика изминилась - на файле 10000 общее потребление снизилось до 143.47 MB
4. Точка роста перестала быть проблеммойчя

### Ваша находка №2
1. Следующая точка роста `sessions.select { |session| session['user_id'] == user['id'] }` - потребляет памяти 104.07 MB на 10000 строк
2. Поскольку метод селект каждый раз создает новый массив, а старый оставляет без изминений, написал собственный метод фильтрации

```ruby
class Array
  def custom_filter!
    i = 0
    acc = []
    while i < self.size
      if yield(self[i])
        acc << self[i] 
        self.delete(self[i])
        next
      end
      i += 1
    end
    acc
  end
end
```

3. Алокация снизилась до 29.99 MB на 10000 строках
4. Данная проблемма перестала быть точкой роста

- какой отчёт показал главную точку роста
- как вы решили её оптимизировать
- как изменилась метрика
- как изменился отчёт профилировщика

### Ваша находка №3
1. Отчет `stack_prof` показывает что теперь алоцированно много объектов и памяти при парсинге даты и разбитие строки метод `split`
2. Убрал парсинг даты так как она уже в нужном формате, `split` при обходе строк заменил на `line =~ /user/` 
3. алокация на 10000 строк уменьшилась до 10 MB
4. Данная проблемма перестала быть точкой роста

### Ваша находка №4
1. отчет `memory prof` показал что теперь много памяти алоцируется при чтении файла и при создании промежуточных объектов пользователя
2. Сделал стриминговое чтение данных(читаю файл по строчно), и не создаю промежуточных объектов пользователя
3. Потребление памяти снизилось до 65.42 MB на 100_000 строк
4. Данная проблемма перестала быть точкой роста
- какой отчёт показал главную точку роста
- как вы решили её оптимизировать
- как изменилась метрика
- как изменился отчёт профилировщика

### Ваша находка №5
1. отчет `memory pro` и вотчер показывает что осноную память программа расходует когда накапливает данные и десеарилизует их затем в JSON(на большых данных 1340 MB)
2. Решил сделать программу полностью потоковой, то есть сразу писать в результирующий отчет и максимум накапливать данные по одному пользователю
3. Потребление памяти снизилось до константног - на полный отчет тратится всего 27 MB, так же увеличилась скорость 12.9!!!!! c за полный отчет
4. Данная проблемма перестала быть точкой роста и мы уложились в бюджет


## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 1400 MB до 27 MB и уложиться в заданный бюджет.

1. Увиличилась скорость выполнения по сравнению с первым заданием до 13 с не смотря на большое количество IO операций

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы написал перформанс тест который проверяет что программа потребляет не более 70 MB памяти
