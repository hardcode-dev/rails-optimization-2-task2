# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику:

Сформировал файл data16000.txt с кол-вом 16_000 строк. В итоге, при парсинге этого файла, кол-во потребляемой памяти равняется 105мб
(проверка проводилась командой `ps -o rss= -p PID`)

Запускать data_large.txt я не решился, да и особо смысла не было, так как мы должны упереться в потребление 70мб при обработке  3250940 строк кода. А у нас при обработке 16000 уже вышло за пределы бюджета

Соответственно, работы будут проводиться на файле состоящим из 16000 строчек

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений

Вот как я построил `feedback_loop`:

- Профилирование
- Обнаружение точки роста
- Оптимизация
- Запуск тестов, чтобы убедиться, что программа возвращает нужный результат(работает верно)
- Метрика (использование команды `ps -o rss= -p PID` по итогу использования программы)


## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался memory_profiles, ruby-prof и valgrind

Вот какие проблемы удалось найти и решить

### Ваша находка №1
Воспользовался гемом memory_profiles, используя файл data16000.txt

Результаты таковы
```
Создано объектов
579055  String
112640  Array
70980   Hash

Потреблено памяти
1.06 GB   Array
26.91 MB  String
15.38 MB  Hash
```

Было определено, что большую часть всего отъедает наполнение массива sessions, а также выборка из него для каждого пользователя.
Также, мы видим большое кол-во созданных объектов String

Посмотрел на то, как собирается массив session ```(sessions + [parse_session(line)])```. Было решено заменить на ```sessions << parse_session(line)``` — то есть, мы наполняем существующий массив, а не создаем каждый раз новый, используя оператор +

Также, добавил магический комментарий ```# frozen_string_literal: true``` дабы зафризить все строки, чтобы каждый раз не создавались новые объекты

После такой небольшой оптимизации, при запуске профилировщика memory_profiler — отчет показал следующее
```
Было создано объектов
437097 String
85512  Array
70980  Hash


И было потребление памяти
322.64 MB Array
20.51 MB  String
15.38 MB  Hash
```

Команда `ps -o rss= -p PID` показала потребление памяти в 54mb

И мы видим, как небольшая оптимизация, позволило существенно сократить потребление памяти

### Ваша находка №2
При последующем профилирование, выявляются почти те же проблемы, что и в предыдущем ДЗ.
Чтение всех sessions по нескольку раз, избыточные map, формирование дублирующих браузеров, когда нужны уникальные и прочее.

При оптимизации всех этих проблем, изменение хранение и сбор статистики(на основе первого ДЗ) - результаты следующие

```
Потребление памяти
11.64 MB  String
6.37 MB   Hash
5.84 MB   Array

Кол-во созданных объектов
215393  String
39874   Array
23315   Hash
```

Кол-во затраченной памяти по команде `ps -o rss= -p PID` - 33 MB
Это много, учитывая, что это всего 16_000 строк из более чем 3_000_000.
Но все же пробуем запустить весь файл(data_large.txt) и посмотреть, сколько он затрачивает памяти

Команда `ps -o rss= -p PID` — показывает 1337 MB

### Ваша находка №3
А теперь самое время перевести все в построчное чтение и построчную запись в результатирующий файл result.json

Также, использовался Set для браузеров
Итак, плоды этой оптимизации
```
Потребление памяти
12.32 MB  String
6.82 MB   Hash
5.78 MB   Array

Кол-во созданных объектов
211569  String
42308   Array
28184   Hash
```

Как видим, по этим параметрам нет никаких изменений. Но если запустить команду `ps -o rss= -p PID` и посмотреть, сколько было затрачено память, то для 16_000 строк показывает - 26 MB

Если же мы запустим с файлом data_large.txt, то показывает те же самые 26 MB

Но если мы посмотрим через визуализатор, то увидим такую картину

![visualize first](https://i.ibb.co/k3FPzwD/first-graph.jpg)

То есть, потребление памяти все же растет…

### Ваша находка №4
Смотрим через ruby-prof CallTree и видим, что проблема в `String#split`, который вызывается в методах `parse_session` и `parse_user`. А также еще кушает трансформация hash в json

Пробуй поставить гем Oj и трансформировать в json через эту либу.
И результаты оказались вполне хорошими.

На обработку полного файла команды `ps -o rss= -p PID` показывает 22 MB

Да и график улучшился
![visualize second](https://i.ibb.co/1Xyrmmy/second-graph.jpg)

### Ваша находка №5
Профилируем дальше

В итоге нам показывается, что проблема также имеется в `parse_session` `String#split`. 

Тут мне пока в голову ничего не приходит, как оптимизировать, поэтому смотрю на остальных кандидатов.

Следующим был метод `collect_stats_from_user`. В нем есть методы `strip, upcase, sort, join и to_i`. Попробуем тут оптимизировать и использовать методы, которые не создают доп объекты. 

Используем `strip!, upcase!, sort!, reverse!`

В итоге, по графику ситуация улучшилась совсем немного

![visualize third](https://i.ibb.co/xmzmjq2/third-graph.jpg)

Итак, при повторном запуске профилировщика, самая большая проблема это в `parse_session` - `sessions.split(',')`. 

Но тут, наверное, пока ничего не сделать.

К сожалению, сделать график более ровным мне не удалось

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с более 1GB потребляемой памяти до 22mb и уложиться в бюджет

А сам скрипт, по benchmark, выполняется за 15 секунду. До оптимизации с памятью, скрипт выполнялся за 26.08 секунд (с первого ДЗ)

Научился пользоваться профилировщиками по затрате памяти, а также получать метрику по памяти.

Есть где задействовать на работе =)

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы - я написал тест, который получает кол-во затраченной памяти, возвращаемый  методом work (`ps -o rss= -p PID`) и сравниваю его в rspec
