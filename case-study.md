# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.
Необходимо было обработать файл с данными, чуть больше ста мегабайт.
У нас уже была программа на `ruby`, которая умела делать нужную обработку.
Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: преобразование текстового файла в файл json предложенного формата с потреблением программой менее 70Мб оперативной памяти на всем протяжении работы.

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop

Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за 2-3 минуты:

-произвести замер времени обработки файла и количества потребляемой программой памяти
-произвести профилирование и выявить точку роста
-попытаться оптимизировать найденную точку роста преобразовав код
-выполнить тестирование корректности и скорости обработки файла
-произвести повторный замер времени и памяти, чтобы убедиться, что оптимизация помогла
-зафиксировать новое максимальное количество памяти в тесте.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы эффективно работать над оптимизацией, я предварительно нарезал исходный файл на файлы, содержашие 10_000, 100_000б 500_000 и 1_500_000 строк, чтобы по мере улучшения метрики программы использовать все более и более большой файл.

Для того, чтобы найти "точки роста" для оптимизации я воспользовался библиотеками memory-profiler, rubyprof в режимах профилирования памяти и аллокаций, stackprof, а так же инструментом valgrind massif, позволяющео оценить количество потребляемой программой памяти на всем протяжении ее работы.

Вот какие проблемы удалось найти и решить

### Подготовка программы к оптимизации
Перед тем, как начать профилировать программу, я переписал ее в потоковый режим работы с исходным файлом, т.к. исходный файл для обработки уже превышает размер метрики и не позволяет снизить лимит потребления оперативной памяти при загрузке файла целиком. Я постарался сразу перенести улучшения полученные в ходе оптимизации времени обработки файла (как например устранение дублирующих `upcase`ов и излишнего парсинга даты). Переписанную программу проверил тестом, чтобы удостовериться, что я не нарушил ее работу.


### Находка №1
- самой инетересной находкой было то, что я сразу добился максимально занимаемого объема оперативной памяти в концу работы программы в 37Mb о чем мне сообщил valgrind-massif-visualiser. Но время обработки файла оставляло желать лучшего. Примерно половина исходного файла обрабатывалась 60 секунд. Поэтому я решил продолжить профилирование в надежде улучшить и этот показатель.

### Находка №2
- memory-profiler показал, что при обработке 100_000 строк больше всего памяти (153.68 MB) тратится в строке `File.write('result.json', "\"#{@full_name}\":#{user_stats[@full_name].to_json}#{closing_char}", mode: 'a')` откуда был сделан вывод, что либо хэш `user_stats` занимает так много памяти, либо метод `to_json` слишком расточительно переводит хэш в джейсон.
- я решил заменить библиотеку `json` на более оптимизированную `oj`
- метрика изменилась с 560 MB до 474 MB, что совсем не мало для файла в 100_000 строк. Изменение решено было оставить
- профилировщик продолжал показывать максимальное потребление памяти в этой строке.

### Находка №3
- memory-profiler снова показал, что при обработке 100_000 строк больше всего памяти тратится в строке `File.write('result.json', "\"#{@full_name}\":#{Oj.dump(user_stats[@full_name])}#{closing_char}", mode: 'a')`
- я решил упростить хеш внутри этой функции, убрав лишний уровень вложенности. но это не дало результата. Тогда я подумал, что возможно `File.write` каждый раз открывает файл `result.json` и пишет в него. Я решил переписать прогрумму, чтобы файл открывался лишь в начале работы, данные писались в него в процессе и файл закрывался, после обработки всех данных.
- метрика изменилась до 166.10 MB, и что немаловажно, скорость обработки файла увеличилась в 2 раза. Обработка полного файла стало занимать 38 секунд и я решил продожить
- профилировщик больше не показывал максимальное потребление памяти в этой строке.

### Находка №4
- RubyProf в режиме MEMORY, показал, что 37% занимаемой памяти приходится на метод `parse_session` и вызов `gsub` внутри метода, который отрезал символ перевода строки у даты.
- я решил попробовать воспользоваться методом `chomp`, что дало 25 секунд на полную обработку файла и
- профилировщик больше не показывал максимальное потребление памяти в этой строке.

### Дальнейшие поиски
Чтобы потреннироваться работать с остальными прфилировшиками я проанализировал отчеты RubyProf в режиме ALLOCATIONS. Удобнее всего мне показался CallStack, на нем я увидет три значимых метода `IO.foreach`,`IO.open` и `IO.write`. Graph показал ту же картину. Профилирование с помошью Stackprof дало увидеть, что внутри `File.foreach` самая тяжелая операция это метод `split` преобразующий строку в массив, но оптимизировать его не представлялось возможным.


## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с нескольких сотен мегабайт (было невозможно проверить метрику для полного файла на неоптимизированной программе, без потокового чтения и записи) до  и уложиться в заданный бюджет.

*Какими ещё результами можете поделиться*

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы *о performance-тестах, которые вы написали*
