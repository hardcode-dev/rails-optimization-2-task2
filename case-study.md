# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: потребляемую память в мегабайтах

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`,
который позволил мне получать обратную связь по эффективности сделанных изменений за ~10 секунд

Вот как я построил `feedback_loop`:
- измерил память выделяемой программой с входными данными размером в 10000 строк через команду ps: 343 МБ
- написал performance-тест потребления памяти
- далее использовал эти же входные данные для дальнейшего профилирования и сравнения метрик

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался:
- memory_profiler
- rubprof
- stackprof

Вот какие проблемы удалось найти и решить:

### Ваша находка №1
- memory-profiler показал что больше всего объектов (110630) создается в строке с парсингом дат
- убрал лишние вызов .map и Date.parse
- метрика уменьшилась с 343 до 336 МБ
- в memory-profiler количество объектов на этой строке снизилось до 5974

### Ваша находка №2
- stackprof показал что больше всего объектов (206928) создается в цикле парсинга строк (file_lines.each ...)
- переписал программу на построчное чтение файла
- метрика уменьшилась с 343 до 30 МБ
- в stackprof уменьшилось количество samples: с 362971 до 241191

### Ваша находка №3
- rubyprof в режиме flat показал что больше всего объектов (136928) объектов создается методом String#split 
- убрал лишние вызовы этого метода
- метрика уменьшилась с 30 до 25 МБ
- в rubyprof уменьшилось количество объектов создаваемых этим методом: с 136928 до 68464

### Ваша находка №4
- memory-profiler показал что создается много одинаковых строк при сравнении
- добавил комментарий frozen_string_literal: true
- метрика уменьшилась с 25 до 24 МБ
- количество создаваемых строк уменьшилось c 133834 до 100759

### Ваша находка №5
- memory-profiler показал что создается 6144 объектов при вычислении user_key
- заменил конкатенацию строк на интерполяцию
- метрика не изменилась
- количество создаваемых объектов уменьшилось c 6144 до 1536

### Ваша находка №6
- memory-profiler показал что создается 4438 объектов при сортировке браузеров
- заменил .sort на .sort!, а так же остальные возмоные методы на bang-методы
- метрика уменьшилась с 24 до 23 МБ
- количество создаваемых объектов при сортировке уменьшилось c 4438 до 2902

### Ваша находка №6
- memory-profiler показал что аллоцируется 1.42 MB при парсинге сессий
- убрал создание хешей при парсинге сессий и юзеров
- метрика уменьшилась с 23 до 21 МБ
- выделенная память у хэшей уменьшилась с 2 МБ до 316 КБ

### Ваша находка №7
- memory-profiler показал что аллоцируется 4.34 MB при split'е строки
- добавил блок в split, заполняя данные по колонкам, без создания промежуточных массивов. Получилось низкоуровнево и почти не читаемо =)
- метрика уменьшилась с 21 до 19 МБ
- выделенная память при сплите уменьшилась с 4.34 до 2.34 МБ, количество созданных массивов уменьшилось с 15807 до 5807

### Ваша находка №8
- memory-profiler показал что аллоцируется 464.56 kB при записи json'a в файл
- переписал на многоступенчатую запись в файл по мере прохождения парсинга
- метрика почти не изменилась
- выделенная память для строк немного увеличилась с 4.12 до 4.61 МБ, однако пиковое потребление памяти в valgrind-massif
  для data_large.txt снизилось с 1.1 ГБ до 389 МБ

### Ваша находка №9
- rubyprof-callgrind показал что 109320 байт памяти выделяется при вызове sort!, из всех bang-методов это самый прожорливый
- переписал аггрегацию браузеров, чтобы сразу сохранялись уникальные, малый массив проще сортировать.
  Также установил версию руби 3.0.1 с jemalloc
- метрика почти не изменилась
- отчет rubyprof-callgrind не изменился почти никак, зато valgrind-massif показал что пиковое потребление памяти
  для data_large.txt снизилось с 389 МБ до 37 МБ!

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 343 до 20 МБ для файла в 10000 строк и уложиться в заданный бюджет.

Профилировщики слабо помогали на 2 последних этапах, приходилось прикидывать в голове какие объекты накапливаются и не освобождаются
сборщиком мусора.

Также, я сравнил ips между текущей оптимизацией и оптимизацией из прошлой ДЗ, получилось:
3.609  (± 2.8%) i/s против 5.199  (± 0.6%) i/s

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы были написаны performance-тесты на потребление памяти
