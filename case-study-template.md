# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: ruby-prof(CallTree) / memory_profiler

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за *время, которое у вас получилось*

Вот как я построил `feedback_loop`: 
- проверить потребляемую память после выполнения с помощью `puts "MEMORY USAGE: %d MB" % (`ps -o rss= -p #{Process.pid}`.to_i / 1024)`
- проверить потребляемую память после выполнения с помощью memory_profiler / ruby-prof calltree
- поиск главной точки роста
- изменение кода

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался memory_profiler / ruby-prof calltree

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- Обычный замер памяти, memory_profiler
- Решено переделать парсинг файла на стриминговый подход
- Память во время выполнения на большом файле снизилась c 500мб до 200мб, после выполнения на среднем файле с 42 до 40мб
- Выделенная память уменьшилась с 283862187 до 283759474 bytes, видимо, из-за file_lines, кол-во аллоцированных объектов уменьшилось на 2(стало 3448461)

### Ваша находка №2
- memory_profiler показал, что на 141 строке выделено большо всего объектов(Date.parse)
- Удаляем ненужный парсинг даты и создание объектов из-за map
- Память после выполнения не изменилась
- Кол-во аллоцированных объектов уменьшилось до 392402(на 870%), выделенная память 68224078(на 416%)

### Ваша находка №3
- rubyprof flat показал, что 82% аллокаций(324480) приходится на String#upcase, так же попробовал graphqhtml, callstack(разделил upcase на 2 части: по 64% и 17%), stackprof показал время выполнения кусков кода
- убрал upcase во всех местах, добавил его при сборке сессий в parse_session
- кол-во аллокаций упало до 1950(на 79.19%)
- кол-во выделяемой памяти упало до 53263718(на 21.9%)

### Ваша находка №4
- rubyprof flat показал, что 31980(46.02%) аллокаций приходятся на split, graphhtml показал, что это происходит внутри метода foreach
- убрал split из parse_user и parse_session, передавая напрямую массив
- кол-во аллокаций split упало до 15990(29.89%), общее кол-во до 53497
- выделяемая память упала до 52249718

### Ваша находка №5
- memory_profiler показал, что больше всего памяти потребляет строка `sessions = sessions + [parse_session(cols)] if cols[0] == 'session'`(15373752 байт) 
- в users и sessions решено добавлять объекты через <<, чтоб не создавать новые
- общее кол-во аллокаций упало до 48809(на 9%), память до 36256046(на 69%)

### Ваша находка №5
- memory_profiler показывает, что больше всего памяти занимают методы с map
- убрал to_i в parse_session, по остальным нет идей
- общее кол-во аллокаций упало до 48809(на 9%), память до 32168766(на 12%)

### Ваша находка №6
- memory_profiler показал много аллокаций(10920) в строке `user_key = "#{user.attributes['first_name']}" + ' ' + "#{user.attributes['last_name']}"`
- сделал строку без +
- общее кол-во аллокаций упало до 39838(на 19%), память до 31841126

### Ваша находка №7
- переделал в полностью стримовый подход
- память уменьшилась до 5149108 (на 84%), теперь можно дождаться выполнения файла на большом объеме данных

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с ~500мб в режиме работы до 200мб и на файле меньше объема с 46мб после выполнения до 39мб и уложиться в заданный бюджет.

Понял, что наиболее эффективны memory_profiler, ruby-prof(flat, calltree), преимущественно их собираюсь в дальнейшем использовать, stackprof оказался бесполезен
Также, сначала искал точки роста по аллокациям, но после 3 находки понял, что искать надо в первую очередь по памяти, потому что все зависит от объема объектов, а не их кол-ва

## Защита от регрессии производительности
Для minitest не нашел подходящего решения, на rspec не хватило времени