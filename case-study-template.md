# Case-study оптимизации

## Актуальная проблема
В нашем проекте возникла серьёзная проблема.

Необходимо было обработать файл с данными, чуть больше ста мегабайт.

У нас уже была программа на `ruby`, которая умела делать нужную обработку.

Она успешно работала на файлах размером пару мегабайт, но для большого файла она работала слишком долго, и не было понятно, закончит ли она вообще работу за какое-то разумное время.

Я решил исправить эту проблему, оптимизировав эту программу.

## Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику: программа должна уложиться в 70мб потребления памяти

## Гарантия корректности работы оптимизированной программы
Программа поставлялась с тестом. Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

## Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений за несколько секунд.

Вот как я построил `feedback_loop`:
1) проверка профайлерами (memory_profiler и ruby-prof)
2) Метрика `(ps -o rss= -p #{Process.pid}.to_i / 1024)`
3) изучение полученных данных и нахождение точек роста
4) Исправление кода
5) прогон теста.

## Вникаем в детали системы, чтобы найти главные точки роста
Для того, чтобы найти "точки роста" для оптимизации я воспользовался memory_profiler и RubyProf

Вот какие проблемы удалось найти и решить

### Ваша находка №1
- Отчет: memory_profiler
- Отчет показал, что главная точка роста находится при добавлении элементов в массив sessions и users,
  а работа всей программы (на 10к записей) съедает 426 МБ
- Объемы потребляемой памяти уменьшились с 420 до 122 МБ
- Этой точка роста в отчете исчезла.

### Ваша находка №2
- На переписывание программы на потоковый стиль указала подсказка из README :)
  А так же в эту итерацию вошли:
  - избавление от лишних split
  - мелкие правки, типа замены строк на символы или добавление констант.
  Но эти фиксы большой роли не сыграли.
- потребление памяти снизилось до 16 МБ (как на прод файле, так и на 10к записей)
- Профилировщик (memory_profiler) не показывает значительных точек роста, больше всего жрет память одинокий split, но так как в бюджет укладываюсь, то решил его пожалеть.

## Результаты
В результате проделанной оптимизации наконец удалось обработать файл с данными.
Удалось улучшить метрику системы с 426 МБ до 16 МБ (пример на 10к записей) и уложиться в заданный бюджет.

Общее время выполнения программы составило 27.73775, что, кстати, превышает время выполнения програмымы на прошлой неделе.

## Защита от регрессии производительности
Для защиты от потери достигнутого прогресса при дальнейших изменениях программы были написаны несколько тестов:
1) тесты для защиты логики программы, которые проверяют корректность сохраненных данных
2) тесты для проверки время выполнения программы
3) тесты для проверки памяти программы.
